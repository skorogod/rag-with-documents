version: '3.8'

services:
  # --- 1. LLM Infrastructure (Ollama) ---
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
    networks:
      - app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  qdrant:
    image: qdrant/qdrant:latest
    container_name: rag_qdrant
    networks:
      - app
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: always

  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: chat_db
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks: 
      - app
  
  client:
    build:
      context: ./Client
    ports:
      - "5173:5173"
      - "24678:24678"
    depends_on:
      - backend
    env_file: .env
    environment:
      - CHOKIDAR_USEPOLLING=true
      - WATCHPACK_POLLING=true
    stdin_open: true
    tty: true
    command: npm run dev -- --host

  backend:
    image: ts-backend
    build: ./Server
    env_file: .env
    ports:
      - "8000:8000"
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
      DB_USERNAME: user
      DB_PASSWORD: password
      DB_DATABASE: chat_db
    networks: 
      - app
    depends_on:
      postgres:
        condition: service_started
      ollama:
        condition: service_started

volumes:
  postgres_data:
  qdrant_storage:

networks:
  app:
    driver: bridge